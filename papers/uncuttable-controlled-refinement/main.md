---
title: "\"Uncuttable\" as Controlled Refinement"
author: "A. Rivero and A.I.Scaffold"
date: "2026"
abstract: |
  This note fixes a meaning of "uncuttable" aligned with the refinement-compatibility thesis of the cornerstone manuscript.
  Here **uncuttable** does not mean "indivisible." It means: the quantity of interest is not determined by any *finite* dissection alone; it is a **limit object** whose definition requires a refinement rule and a comparison structure across refinements.
  
  The point is structural and mathematical: once a theory is built from composable local pieces, the continuum theory is the stable target of a refinement limit, and extra control data may be required for that limit to exist or be unique.
---

# 1. Definition
Call a quantity \(Q\) **uncuttable** (in this note's sense) if:
1. there exists a family of finite approximants \(Q_N\) produced by a finite dissection/refinement scheme of depth \(N\), but
2. the value of interest is not any finite \(Q_N\); it is a controlled limit \(Q=\lim_{N\to\infty}Q_N\), and
3. specifying the *rule of refinement* and the *comparison across refinements* is part of the definition of \(Q\).

The historical resonance is deliberate. The Greek \(\acute\alpha\tau o\mu o\varsigma\) ("a-tomos," uncuttable) was coined by Leucippus and Democritus to denote indivisible substance — matter that cannot be divided further. The shift proposed here is from ontology to procedure: what is "uncuttable" is not a smallest piece of stuff, but a limit object that no single finite dissection captures. The indivisibility is not in the substance but in the definition: you cannot "cut" the limit into finitely many pieces and recover it without specifying how the pieces are to be reassembled and refined.

This is the ordinary situation in analysis: finite partitions approximate, but the object is defined by a limiting procedure together with hypotheses that ensure convergence/uniqueness.

`Remark 1.1 (Cauchy completion: the foundational archetype).`
The real numbers are the prototypical "uncuttable" limit object. The rationals \(\mathbb{Q}\) provide finite approximants (each a ratio of integers); a real number is the equivalence class of Cauchy sequences of rationals, with the metric simultaneously defining Cauchy convergence and the identification of sequences with the same limit (Méray, 1869; Cantor, 1872). This matches the present definition: finite approximants exist (item 1), the value is the controlled limit (item 2), and the metric is the refinement rule (item 3). Strikingly, the same rationals completed under different metrics yield different objects — the \(p\)-adic completions \(\mathbb{Q}_p\) — so the "control data" (choice of absolute value; classified by Ostrowski's theorem) genuinely determines which limit object emerges. The same completion pattern recurs throughout: the Riemann integral (Section 2) as a directed limit of sums over partition refinements; the quantum propagator (Section 3) as a Trotter-product refinement limit (Remark 3.3); and, heuristically, the continuum QFT (Section 4) as the RG-controlled limit of lattice theories — though no rigorous metric on theory space is generally available for the last case. Completion can also introduce structure absent from all finite stages: the reals are uncountable (Cantor's diagonal argument) and contain transcendentals, properties that no finite collection of rationals exhibits; similarly, continuum QFTs can have anomalies, non-perturbative sectors, and phase transitions absent from any finite lattice approximant.

`Remark 1.2 (Zeno's paradoxes as proto-refinement limits).`
The etymological note above connects "uncuttable" to the Greek atomists. A deeper connection runs through Zeno of Elea (5th c. BCE), whose paradoxes of motion — recorded by Aristotle (*Physics* VI) — were *reductio ad absurdum* arguments defending Parmenides' monism: if one accepts plurality and infinite divisibility, motion leads to contradictions. In the dichotomy paradox, reaching a destination requires first traversing half the distance, then half the remainder, and so on — in modern terms, the geometric series \(1/2 + 1/4 + 1/8 + \cdots = 1\), though this formalization is anachronistic (rigorous series summation awaited Cauchy, 1821, and the epsilon-delta framework of Weierstrass, 1860s). The Achilles paradox has the same structure with a speed-dependent geometric ratio. The arrow paradox is different: at each durationless instant the arrow occupies a fixed position, so how can it move? Aristotle's own response was that time is a continuum, not composed of indivisible nows; the modern resolution defines velocity as the derivative — a refinement limit (Example 2.1) — rather than a property of any single instant. All three paradoxes are resolved by specifying convergence structures (series limits, derivatives) that the Greek formulation lacked. The historical responses partition neatly: the atomists eliminated infinite regression by positing a physical lower bound on division (indivisible atoms); modern analysis keeps the infinite divisibility but adds convergence control — exactly the "controlled refinement" of Section 1's definition. This arc — from Eleatic paradox through atomist truncation to analytic limit theory — suggestively parallels the paper's progression from finite approximants (item 1) through controlled limits (item 2) to the specification of refinement rules (item 3).

`Remark 1.3 (Dedekind cuts versus Cauchy sequences: two refinement procedures, one limit object).`
Remark 1.1 constructs the reals via Cauchy completion — an inherently metric procedure. Dedekind (1872) gave a structurally different construction: a *Dedekind cut* is a partition of \(\mathbb{Q}\) into non-empty sets \((L,U)\) with every element of \(L\) less than every element of \(U\) and \(L\) having no greatest element. The real numbers are the set of all such cuts, ordered by inclusion of lower sets, with arithmetic defined by corresponding operations on cuts. Crucially, no metric or limit concept appears — the construction is purely order-theoretic, relying on the completeness property (every non-empty bounded subset has a supremum) rather than on Cauchy convergence. Yet the two constructions yield the same complete ordered field, and this is not accidental: any two complete ordered fields are isomorphic by a *unique* order-preserving field isomorphism — the completeness axiom is rigid enough to force uniqueness. In the language of Section 1's definition, the "control data" (item 3) takes entirely different forms — a metric in one case, a linear order in the other — while the "uncuttable" limit object (item 2) is the same. The two procedures also generalize differently: Cauchy completion works in any metric space (yielding \(p\)-adic completions \(\mathbb{Q}_p\), Banach spaces, etc.), whereas Dedekind completion applies to any totally ordered set (producing a complete lattice) but does not extend to settings without a total order [Dedekind1872]. This complementarity — different refinement paths converging to the same target when uniqueness conditions are met — recurs in Section 3 (Stone–von Neumann uniqueness, Remark 3.8) and in Section 4 (dualities, Remark 4.8).

# 2. Toy model: an integral is already a refinement limit
Let \(f:[a,b]\to\mathbb R\). A prototypical refinement family is a partition
\(P_N=\{a=t_0<\cdots<t_N=b\}\) with mesh \(\|P_N\|:=\max_k(t_{k+1}-t_k)\to 0\).
Define the Riemann-sum approximants
\[
Q_N:=\sum_{k=0}^{N-1} f(\xi_k)\,(t_{k+1}-t_k),
\qquad \xi_k\in[t_k,t_{k+1}].
\]
In good cases, \(Q_N\to \int_a^b f(t)\,dt\) as \(\|P_N\|\to 0\), and the limit is independent of the tags \(\xi_k\). But this is not a tautology: the limit can fail to exist or can depend on the refinement rule unless hypotheses are stated.

A concrete failure case appears already in calculus. Consider the difference quotient \(f(x+\varepsilon)/\varepsilon\): for each finite \(\varepsilon\), the quantity diverges as \(\varepsilon\to 0\). The "refinement limit" exists only after subtracting a counterterm \(f(x)/\varepsilon\), yielding the derivative \(f'(x)\). The subtraction is part of the definition of the limit — without it, the refinement procedure does not converge. This is the simplest model of renormalization: a "cut" at finite \(\varepsilon\) is not the answer; the answer requires a controlled \(\varepsilon\to 0\) limit with explicit subtraction rules.

In the present program, this is the basic moral:
finite cuts approximate, but the value is defined by **controlled refinement**.

## 2.5 Worked example: the derivative as a renormalized refinement limit

The paragraph above describes the difference-quotient subtraction informally. We now make it explicit as a minimal model of renormalization.

`Example 2.1 (Derivative as counterterm-subtracted limit).`
Let \(f:\mathbb{R}\to\mathbb{R}\) be differentiable at \(x\). The naive "refinement" approximant at scale \(\varepsilon\) is
\[
R(\varepsilon):=\frac{f(x+\varepsilon)}{\varepsilon}.
\]
This diverges as \(\varepsilon\to 0\) whenever \(f(x)\neq 0\). To extract a finite limit, subtract a counterterm:
\[
R_{\mathrm{ren}}(\varepsilon):=\frac{f(x+\varepsilon)-f(x)}{\varepsilon}.
\]
Now \(\lim_{\varepsilon\to 0}R_{\mathrm{ren}}(\varepsilon)=f'(x)\), which is finite and independent of the "regulator" \(\varepsilon\). The structure is:

1. **Regulated quantity**: \(R(\varepsilon)\), finite for each \(\varepsilon>0\) but divergent as \(\varepsilon\to 0\).
2. **Counterterm**: \(f(x)/\varepsilon\), which absorbs the divergence.
3. **Renormalized observable**: \(f'(x)\), the \(\varepsilon\)-independent limit.

This is formally identical to the pattern in perturbative renormalization: a bare quantity diverges as the regulator is removed, but a systematic subtraction yields a finite, physically meaningful result. The subtraction rule is not ad hoc; it is forced by the requirement that the result depend smoothly on the data and be independent of the regulator.

`Remark 2.2 (Higher-order counterterms).`
If \(f\) is \(C^n\), the Taylor expansion
\[
f(x+\varepsilon)=f(x)+f'(x)\varepsilon+\tfrac{1}{2}f''(x)\varepsilon^2+\cdots+\tfrac{1}{n!}f^{(n)}(x)\varepsilon^n+o(\varepsilon^n)
\]
provides a systematic tower of subtractions, one per order. Removing terms through order \(k-1\) and dividing by \(\varepsilon^k\) gives the \(k\)-th Taylor coefficient \(f^{(k)}(x)/k!\) plus vanishing corrections — or equivalently, dividing by \(\varepsilon^k/k!\) recovers the derivative \(f^{(k)}(x)\) itself. Each order is a "counterterm-subtracted refinement limit" — the analog of loop-by-loop renormalization in QFT.

`Remark 2.3 (Euler–Maclaurin: correction tower for the Riemann sum).`
The same pattern applies to the integral approximation itself. The Euler–Maclaurin formula expresses the error of the left-point Riemann sum as a systematic expansion in the mesh size \(h=(b-a)/N\):
\[
\sum_{k=0}^{N-1}f(a+kh)\,h = \int_a^b f(t)\,dt + \sum_{j=1}^{p}\frac{B_j}{j!}\,h^j\,\bigl[f^{(j-1)}(b)-f^{(j-1)}(a)\bigr] + O(h^{p+1}),
\]
where \(B_j\) are Bernoulli numbers (\(B_1=-\tfrac12,\;B_2=\tfrac16,\;B_3=0,\ldots\)). Each correction involves a power of the mesh (the "regulator"), multiplied by endpoint derivatives (boundary data). The leading correction \(j=1\) converts the left-point sum into the trapezoidal rule; the \(j=2\) term adds an endpoint-derivative correction that raises the order to \(O(h^4)\). This is the interval counterpart of Remark 2.2's single-point tower: there the subtractions were local (Taylor coefficients at \(x\)); here they are boundary-localized (endpoint derivatives of \(f\)).

`Remark 2.4 (Richardson extrapolation: subtracting leading errors across refinement levels).`
Given an approximation family \(Q(h)=Q+c_1 h^p+O(h^{p+1})\), the combination \(R(h)=(2^p\,Q(h/2)-Q(h))/(2^p-1)\) eliminates the leading error term without knowledge of \(c_1\) — only the exponent \(p\) is needed. Iterating at mesh sizes \(h, h/2, h/4,\ldots\) produces a triangular tableau (Romberg) that peels off one order of correction per column, mirroring the counterterm tower of Remark 2.2. Applied to the trapezoidal rule (whose even-power error expansion follows from Remark 2.3 with \(B_{2k+1}=0\) for \(k\ge1\)), the first Richardson step (\(p=2\)) yields Simpson's rule; the second (\(p=4\)) yields Boole's rule — each time eliminating the next Euler–Maclaurin correction without computing endpoint derivatives. The structural parallel to minimal subtraction in dimensional regularization is precise: the exponent \(p\) plays the role of the pole order, and the subtraction procedure is universal — independent of the specific integrand.

`Remark 2.5 (Padé approximants and Borel resummation: controlled extrapolation beyond convergence).`
The polynomial counterterm towers of Remarks 2.2–2.4 fail when the series itself diverges. Padé approximants — rational functions \([L/M]=P_L(x)/Q_M(x)\) whose Taylor expansion matches a given series through order \(x^{L+M}\) — extend the strategy to meromorphic functions: the Montessus de Ballore theorem guarantees convergence outside the Taylor disk when singularities are isolated poles. For asymptotic (divergent) series with factorial coefficient growth, the Borel transform \(B(t)=\sum (c_n/n!)\,t^n\) has finite radius of convergence, and the Borel–Padé method — Padé-approximating \(B(t)\) and Laplace-transforming back — extracts a finite, well-defined function from a formally divergent expansion. This is the "uncuttable" pattern at the level of series: the naive sum has no limit, but a systematic resummation prescription (transform, rational approximation, inverse transform) is part of the definition of the intended quantity.

`Remark 2.6 (Optimal truncation: the exponential accuracy barrier).`
For an asymptotic (divergent) series whose coefficients grow as \(|a_n|\sim C\,n!/\rho^n\), the \(n\)-th term is minimized near \(n\approx\rho/x\). Truncating at this optimal index — the smallest term — yields an approximation with exponentially small error \(\sim e^{-\rho/x}\), far better than any fixed algebraic order. This "superasymptotic" accuracy (Berry, 1991) is the best any finite truncation can achieve; the exponentially small remainder is precisely the non-perturbative content (instantons, tunneling amplitudes) that the Borel resummation of Remark 2.5 recovers. Optimal truncation thus marks the boundary between what finite "cuts" can reach and what requires the full resummation machinery: the ultimate illustration that the intended quantity is not captured by any single approximant but lives in the controlled limit.

`Remark 2.7 (Zeta-function regularization: finite values without a cutoff).`
The counterterm towers of Remarks 2.2–2.4 and the resummation methods of Remarks 2.5–2.6 all work by explicitly handling a regulator (mesh size, truncation index, Borel parameter). Zeta-function regularization takes a different route: the formally divergent sum \(\sum_{n=1}^\infty n^{-s}\) converges for \(\mathrm{Re}(s)>1\) and extends meromorphically to all \(s\) (with a single pole at \(s=1\)); the analytic continuation value \(\zeta(-1)=-1/12\) is the regularized assignment to the divergent sum \(1+2+3+\cdots\). For a differential operator \(A\) with eigenvalues \(\{\lambda_n\}\), the spectral zeta function \(\zeta_A(s)=\sum\lambda_n^{-s}\) regularizes the functional determinant via \(\ln\det A=-\zeta_A'(0)\) (Ray and Singer, 1971; Hawking, 1977). No cutoff appears; the finite answer is extracted from the analytic structure of the sum across the complex \(s\)-plane. The connection to Section 2's earlier methods is through Bernoulli numbers: \(\zeta(-n)=-B_{n+1}/(n+1)\), the same numbers that control the Euler–Maclaurin corrections of Remark 2.3. In physics, zeta regularization computes the Casimir energy (Remark 5.7 of the fermionic companion) and determines the critical dimension of the bosonic string (\(d=26\)), where the normal-ordering constant involves both \(\zeta(0)=-1/2\) and \(\zeta(-1)=-1/12\). This is the "uncuttable" pattern at its most algebraic: the quantity is defined not by removing a regulator but by the unique meromorphic continuation of a family of convergent sums.

`Remark 2.8 (Resurgence: the trans-series as the ultimate "uncuttable" object).`
The Borel resummation of Remark 2.5 and the optimal truncation of Remark 2.6 extract finite values from divergent series, but they leave an exponentially small ambiguity \(\sim e^{-A/g}\). Resurgence (Écalle, 1981) reveals that this ambiguity is not accidental: the large-order behavior of the perturbative coefficients \(|a_n|\sim C\,A^{-n}\,\Gamma(n+\beta)\) encodes the instanton action \(A\) through the exponential growth rate \(A^{-n}\), while the power \(\beta\) is determined by the fluctuation determinant around the instanton saddle point (Bender and Wu, 1969, 1973). The full answer is a trans-series \(f(g)=\sum_{k\ge 0}\sigma^k e^{-kA/g}\sum_n a_{n,k}\,g^n\) (with logarithmic corrections in general), where each instanton sector \(k\ge 1\) has its own perturbative tail, and the trans-series parameter \(\sigma\) is fixed by Stokes phenomena — discontinuous jumps in the resummation as \(\arg g\) crosses Stokes lines. Écalle's alien calculus provides operators \(\Delta_\omega\) (one per Borel-plane singularity at \(\omega\)) that extract the non-perturbative data from the perturbative series, establishing that the sectors are not independent: the perturbative coefficients "know" the instanton actions, and conversely. In QCD, IR renormalons — Borel-plane singularities on the positive real axis at \(t=2m/\beta_0\) — encode power corrections \(\sim(\Lambda_{\mathrm{QCD}}/Q)^{2m}\) that match the operator product expansion, connecting this circle of ideas to the Wilson RG framework of Remark 4.1. The trans-series is the ultimate "uncuttable" object: no single perturbative sector suffices, and the full answer requires all instanton sectors tied together by resurgence relations.

`Remark 2.8a (Euler–Heisenberg Lagrangian as resurgent-transseries witness).`
The weak-field expansion of the one-loop QED effective action \(\mathcal{L}_{\mathrm{EH}}\) in \(x = (eE/m^2)^2\) has coefficient growth \(a_k \sim C\cdot (2k-1)!/\pi^{2k}\) with \(C \approx 0.2026\) (verified by explicit SymPy computation of the Bernoulli-number representation). This is super-factorial: faster than \(k!\) by the double-factorial factor \((2k)!/(2^k k!\cdot k!) \sim 4^k/\sqrt{\pi k}\). The series is therefore not Borel-summable in the standard sense; the non-perturbative content is instead an essential singularity \(e^{-\pi/z}\) at \(z = eE/m^2 = 0\), corresponding to the Schwinger pair-production rate \(\Gamma \sim \sum_{n=1}^\infty (c_n/n^2)\,e^{-n\pi/z}\) (transseries in the electric field). This is a QFT-level realization of the perturbative-encodes-nonperturbative thesis of Remark 2.8: the sequence \(\{a_k\}\) determines the full resurgent structure including the instanton actions \(n\pi m^2/(eE)\). Note that Borel–Padé applies at the level of the \(\alpha\)-expansion (renormalon series at fixed field strength), not at the level of the weak-field expansion in \(z\).

# 3. Dynamics: action, stationarity, and the need for control data
The cornerstone manuscript uses the same template in mechanics.
Given a partition of time, the discrete action
\[
S_N[q]=\sum_k \mathcal L\!\left(q_k,\frac{q_{k+1}-q_k}{\Delta t_k},t_k\right)\Delta t_k
\]
is a finite refinement approximant. The continuum action \(S[q]=\int \mathcal L\,dt\) is a refinement limit.

Two "uncuttable" features appear immediately when one pushes beyond smooth classical paths:
1. **Singular probes and corners:** stationarity must be interpreted in weak/distributional form; point-supported variations require mollification.
2. **Non-uniqueness of refinement schemes:** different discretization conventions (even if classically equivalent) can produce distinct refined objects unless an equivalence or control map is specified.

These are exactly the obstructions discussed in the cornerstone manuscript: the point is not indivisible atoms, but limit control.

In the quantum setting, the "uncuttable" character becomes sharper. The path-integral amplitude
\[
K(q_f,t_f;q_i,t_i)=\int \prod_{k=1}^{N-1}dq_k\;\prod_{k=0}^{N-1}K_\Delta(q_{k+1},q_k;t_k)
\]
is a product of short-time kernels composed over a time partition of depth \(N\). No finite \(N\) gives the exact propagator; the propagator is the \(N\to\infty\) refinement limit. Crucially, the control parameter \(\hbar\) enters the short-time kernels as \(\exp(iS_\Delta/\hbar)\), and different discretization conventions (left-point, midpoint, symmetric) can produce distinct \(O(\hbar)\) corrections even though they share the same classical \(\hbar\to 0\) limit [FeynmanHibbs1965]. Thus the quantum amplitude is doubly "uncuttable": it requires both a refinement rule (time-slicing prescription) and a comparison/equivalence structure (ordering convention or half-density normalization) before the limit is well-defined.

`Example 3.1 (Non-uniqueness of refinement: \(\alpha\)-ordering).`
For the classical Hamiltonian \(H(q,p)=qp\), different time-slicing prescriptions — evaluating position at \(q_\alpha=(1-\alpha)q_k+\alpha q_{k+1}\) in each slice — produce different quantum operators:
\[
\hat H_\alpha=\alpha\hat q\hat p + (1-\alpha)\hat p\hat q = \hat p\hat q + \alpha\,i\hbar.
\]
At \(\alpha=0\) (prepoint): \(\hat p\hat q\). At \(\alpha=1/2\) (midpoint/Weyl): \(\tfrac12(\hat q\hat p+\hat p\hat q)\). At \(\alpha=1\) (postpoint): \(\hat q\hat p\). All three share the classical limit \(H=qp\) as \(\hbar\to0\), but they are distinct quantum objects. The "uncuttable" message: the continuum quantum Hamiltonian is not determined by any single finite time-slicing; it requires specifying the refinement convention \(\alpha\) as part of the definition.

`Remark 3.2 (Stochastic counterpart: Itô versus Stratonovich).`
The same refinement non-uniqueness appears in stochastic calculus. For a Wiener process \(W_t\), the stochastic integral \(\int f(W_t)\,dW_t\) can be defined using left-point (Itô) or midpoint (Stratonovich) evaluation in the Riemann sums:
\[
\text{Itô:}\;\sum_k f(W_{t_k})\,\Delta W_k,\qquad
\text{Stratonovich:}\;\sum_k f\!\left(\tfrac{W_{t_k}+W_{t_{k+1}}}{2}\right)\Delta W_k.
\]
Both converge as mesh \(\to 0\) (for \(f\in C^2\)), but to different limits related by \(\int f(W)\circ dW = \int f(W)\,dW + \tfrac12\int f'(W)\,dt\) [Oksendal2003]. The correction \(\tfrac12 f'\,dt\) arises because Brownian paths have non-zero quadratic variation (\(\sum(\Delta W_k)^2\to T\neq 0\)); for paths of bounded variation, the quadratic variation vanishes and all evaluation-point prescriptions agree. This parallels Example 3.1: the ordering correction \(\alpha\,i\hbar\) vanishes when \(\hbar\to 0\) (smooth classical paths), but is unavoidable at finite \(\hbar\). Both cases instantiate the "uncuttable" pattern: when paths are rough enough, the refinement prescription becomes part of the definition.

`Remark 3.3 (Trotter product formula as a refinement theorem).`
The mathematical backbone of the path-integral refinement limit is the Trotter product formula: for self-adjoint operators \(A\) and \(B\) with \(A+B\) essentially self-adjoint on a common dense domain,
\[
e^{t(A+B)}=\lim_{N\to\infty}\left(e^{tA/N}\,e^{tB/N}\right)^N.
\]
In the path-integral context, \(A=-i\hat T/\hbar\) (kinetic) and \(B=-i\hat V/\hbar\) (potential), so each factor is a free propagation or a potential phase-kick at one time slice. The formula states that the exact propagator is a refinement limit of \(N\)-fold compositions — and that this limit converges, with total error \(O(1/N)\). The symmetric (Suzuki–Trotter) splitting \(e^{tA/(2N)}e^{tB/N}e^{tA/(2N)}\) reduces the total error to \(O(1/N^2)\) by canceling the leading Baker–Campbell–Hausdorff commutator \([A,B]\) contribution at each step. This is the "uncuttable" pattern in operator-algebraic form: no finite product equals \(e^{t(A+B)}\), and the rate of convergence depends on controlling the non-commutativity of the pieces [FeynmanHibbs1965].

`Remark 3.4 (Symplectic integrators: structural compatibility at finite resolution).`
The symmetric Trotter splitting is the operator form of the Störmer–Verlet (leapfrog) integrator. The discrete map is *exactly* symplectic at every finite \(N\): it preserves the Poisson brackets (or, quantum-mechanically, unitarity) not just asymptotically but at each approximation level. Backward error analysis shows that the numerical flow is the exact flow of a nearby "shadow Hamiltonian" \(\tilde H = H + O(\Delta t^2)\), which is conserved exactly along the discrete orbit; the original \(H\) oscillates with amplitude \(O(\Delta t^2)\) without secular drift. Higher-order symplectic splittings (Yoshida, Forest–Ruth) systematically cancel further Baker–Campbell–Hausdorff commutators, paralleling the counterterm tower of Remark 2.2. This is refinement compatibility made precise: the approximants at every level carry a structural invariant (symplecticity) that the continuum limit inherits.

`Remark 3.5 (Adiabatic limit: Berry phase as geometric refinement correction).`
The adiabatic theorem is another instance of the same refinement pattern. A slowly varying Hamiltonian \(H(t/T)\) with \(T\to\infty\) has instantaneous eigenstates as its finite approximants; the exact evolution is the refinement limit, controlled by the gap condition (energy gaps bounded away from zero). Diabatic transitions are suppressed as \(T\to\infty\), but a geometric correction survives exactly: the Berry phase \(\gamma_n = i\oint\langle n|\nabla_R n\rangle\cdot dR\), which is the holonomy of a U(1) connection on the eigenstate bundle over parameter space. This is the "extra structure" of Section 4 in geometric dress — a connection datum that no local approximant captures, yet persists through the refinement limit and can be topologically quantized.

`Remark 3.6 (WKB approximation: \(\hbar\)-refinement with connection-formula control data).`
The WKB ansatz \(\psi(x)=A(x)\exp(iS(x)/\hbar)\) generates a formal expansion of \(S\) in powers of \(\hbar\), with the leading term satisfying the Hamilton–Jacobi equation (classical mechanics) and each subsequent order adding a quantum correction — a refinement family indexed by truncation depth. At classical turning points, the WKB amplitude \(A\propto 1/\sqrt{p(x)}\) diverges: the local approximation breaks down, and a connection formula (obtained by matching to an Airy solution) is needed to relate oscillatory and evanescent regions. This connection formula is the "control data" for the global WKB solution, and the requirement that the wavefunction be single-valued around a closed orbit produces the Bohr–Sommerfeld quantization condition \(\oint p\,dq=(n+\tfrac12\mu)\,\hbar\), where \(\mu\) is the Maslov index counting turning points — a topological invariant that, like the Berry phase in Remark 3.5, no local WKB expansion can determine.

`Remark 3.7 (Deformation quantization: the star product as \(\hbar\)-refinement).`
The refinement in mesh size (Sections 2–3) has a complement in the parameter \(\hbar\): deformation quantization replaces the pointwise product of classical observables with the Moyal star product \((f\star g)(q,p)=f\cdot g+(i\hbar/2)\{f,g\}+O(\hbar^2)\), where each order in \(\hbar\) adds higher-derivative corrections — the "counterterm tower" of Section 2, now indexed by powers of \(\hbar\). The star commutator \(f\star g-g\star f=i\hbar\{f,g\}+O(\hbar^3)\) recovers the Poisson bracket at leading order (with the \(O(\hbar^2)\) term vanishing by antisymmetry), making quantum mechanics a systematic deformation of classical mechanics. Different operator orderings (Weyl, normal, anti-normal) correspond to different but gauge-equivalent star products — the deformation-quantization counterpart of the \(\alpha\)-ordering non-uniqueness of Example 3.1. Kontsevich's formality theorem (1997; Fields Medal 1998) proved that every Poisson manifold admits a star product, establishing the existence of deformation quantization in full generality and classifying all star products up to equivalence by formal deformations of the Poisson structure.

`Remark 3.8 (Stone-von Neumann uniqueness: when the refinement limit has a unique destination).`
The "uncuttable" pattern of Sections 2–3 raises a natural question: does the refinement limit land in a unique destination? For finitely many degrees of freedom, the Stone-von Neumann theorem (Stone, 1930; von Neumann, 1931) answers affirmatively: every irreducible strongly continuous unitary representation of the Weyl commutation relations for \(n\) canonical pairs \((\hat q_j,\hat p_j)\) is unitarily equivalent to the Schrödinger representation on \(L^2(\mathbb{R}^n)\). The kinematical arena — the Hilbert space carrying the basic observables — is therefore unique, and the \(\alpha\)-ordering non-uniqueness of Example 3.1 is intrinsic to the quantization map (the ordering prescription), not an artifact of choosing a different representation. For infinitely many degrees of freedom (quantum field theory), the theorem fails: the CCR algebra admits inequivalent irreducible representations, and the choice of representation (equivalently, the choice of vacuum state) becomes part of the theory's definition. A dramatic consequence is Haag's theorem (Haag, 1955; Hall and Wightman, 1957): in a Wightman QFT, the free and interacting vacua necessarily live in inequivalent representations, so the interaction picture — which assumes a unitary intertwiner between them — cannot describe genuinely interacting theories. At the non-perturbative level, the vacuum choice is additional "control data" absent in finite-dimensional quantum mechanics; at the perturbative level, even within a fixed Fock representation, counterterms and renormalization conditions (Sections 2 and 4) provide extra data needed to extract finite predictions. Both layers of non-uniqueness are absent for finitely many degrees of freedom, where Stone-von Neumann fixes the kinematics and the dynamics is well-defined once an ordering is chosen.

`Remark 3.9 (Feynman–Kac formula: the Euclidean path integral as a rigorous refinement limit).`
The path-integral refinement limit of Section 3 is formal in Minkowski signature: Cameron (1960) proved that no countably additive measure on path space makes \(\int e^{iS/\hbar}\,\mathcal{D}q\) a Lebesgue integral — the oscillatory phase defeats \(\sigma\)-additivity. The Feynman–Kac formula (Kac, 1949) resolves this by Wick rotation \(t\to -i\tau\): for a potential \(V\) bounded below and continuous, the imaginary-time propagator is
\[
\langle x|\,e^{-\tau H}\,|y\rangle = \mathbb{E}^{W}_{x\to y}\!\left[\exp\!\left(-\int_0^\tau V(B_s)\,ds\right)\right],
\]
where \(B_s\) is a Brownian bridge from \(x\) to \(y\) in time \(\tau\), and \(\mathbb{E}^W\) denotes the Wiener-measure expectation — a genuine \(\sigma\)-additive probability measure on continuous paths \(C([0,\tau])\) [Kac1949]. The formula can be proved using the Trotter product formula (Remark 3.3): splitting \(e^{-\tau(T+V)/N}\) into free and potential factors, the free propagators are Gaussian transition densities that define the Wiener measure in the \(N\to\infty\) limit. This connects the operator-algebraic refinement (Remark 3.3) to the measure-theoretic one. The Wiener measure is also the natural setting for Itô calculus (Remark 3.2), so the Feynman–Kac formula unifies three threads of Section 3: stochastic integration, Trotter composition, and the path integral. In field theory, the same Euclidean strategy underlies constructive QFT (Remark 4.11): the Osterwalder–Schrader axioms — including reflection positivity, the measure-theoretic analog of unitarity — provide the bridge from Euclidean path measures back to Wightman QFTs via Nelson's reconstruction (1973) [Simon1979]. The "uncuttable" message is sharpened: the Minkowski path integral is not merely hard to define rigorously — it *cannot* be a measure. The well-defined limit object requires Wick rotation as additional "control data," and neither the formal Minkowski expression nor any finite time-slicing captures the answer.

# 4. Outlook: refinement compatibility as "the extra structure"
In the companion papers, the "extra structure" used to control refinement limits is made explicit:
- half-densities make kernel composition coordinate-free without hidden measure choices,
- control maps \(\tau\) encode how parameters must flow under refinement to maintain stability,
- renormalization is the compatibility rule when naive refinement limits diverge.

`Remark 4.1 (Wilson's renormalization group: the definitive refinement framework).`
The counterterm towers of Section 2 and the refinement-convention dependence of Section 3 find their definitive home in Wilson's renormalization group (1971, 1974). Wilson's blocking transformation — integrating out short-distance degrees of freedom, rescaling, and absorbing the result into effective couplings — is literally a refinement map on the space of theories. Relevant operators (those that grow under the RG flow) are the counterterms that must be tuned to reach a well-defined continuum limit; irrelevant operators shrink and their effects vanish — this is the origin of universality, the statement that the continuum limit depends only on a few relevant couplings, not on microscopic lattice details. The continuum QFT is not "what you get when the lattice spacing goes to zero" but the theory defined by the vicinity of an RG fixed point, the self-consistent target toward which all sufficiently fine approximants flow. This is the "uncuttable" pattern at full strength: no finite lattice is the theory, but the theory is the stable limit of the refinement flow, defined by the fixed-point structure and the relevant couplings that parameterize departures from it.

`Remark 4.2 (Dimensional regularization: refinement-scheme independence made explicit).`
The counterterm towers of Section 2 use a momentum cutoff \(\Lambda\) as the finite-resolution regulator. Dimensional regularization ('t Hooft and Veltman, 1972) provides a structurally different scheme: loop integrals are evaluated in \(d=4-\varepsilon\) dimensions, and UV divergences appear as poles in \(1/\varepsilon\) rather than as \(\Lambda\to\infty\) power laws. The minimal subtraction prescription (MS-bar) — subtracting the \(1/\varepsilon\) pole together with universal constants \(\ln(4\pi)-\gamma_E\) — is the counterterm convention, analogous to the evaluation-point convention \(\alpha\) of Example 3.1. At \(L\) loops, poles up to \(1/\varepsilon^L\) arise from nested subdivergences, and Zimmermann's forest formula (the BPHZ recursive subtraction scheme) ensures that order-by-order pole subtraction leaves a finite result — the dim-reg version of the counterterm tower in Remark 2.2. Crucially, cutoff and dim-reg regularization produce the same physical S-matrix elements: the renormalized observables are independent of the specific regulator, just as the integral in Section 2 is independent of the partition rule. Dimensional regularization additionally preserves gauge invariance (Ward-Takahashi identities) and Lorentz covariance, structural invariants that a naive momentum cutoff would break — paralleling the symplectic integrators of Remark 3.4, which preserve Hamiltonian structure that generic discretizations destroy.

`Remark 4.3 (Lattice gauge theory: the definitive finite-resolution approximant).`
Wilson's lattice gauge theory (1974) provides the most concrete physical realization of the "uncuttable" pattern. Spacetime is discretized on a hypercubic lattice with spacing \(a\); the gauge field lives on links as group elements \(U_\mu(x)\in SU(N)\), and the action is built from the smallest closed loops (plaquettes): \(S_W=\beta\sum_P\mathrm{Re}\,\mathrm{Tr}(1-U_P)\) with \(\beta=2N/g^2\). Crucially, this formulation maintains exact gauge invariance at every lattice spacing — the action is a sum of traces of closed loops, which are invariant under local gauge transformations — paralleling the symplectic integrators of Remark 3.4, which preserve Hamiltonian structure at every finite time step. The continuum limit \(a\to 0\) requires tuning the bare coupling \(g(a)\) along the critical surface \(\beta\to\infty\); for non-abelian theories with \(\beta_0=(11N-2N_f)/3>0\), asymptotic freedom (Gross–Wilczek and Politzer, 1973) guarantees that \(g(a)\to 0\) as \(a\to 0\), making this tuning controllable. No finite lattice is the theory: the continuum QCD is the refinement limit, and proving that confinement (the area law for large Wilson loops) and a mass gap survive this limit is the content of the Clay Millennium Prize problem for Yang–Mills theory.

`Remark 4.4 (Effective field theory: the systematic refinement hierarchy).`
The counterterm towers, refinement conventions, and RG flows of Sections 2–4 are unified in the effective field theory (EFT) framework (Weinberg, 1979). EFT organizes physical predictions as a tower of Lagrangians \(\mathcal{L}_{\mathrm{EFT}}(\Lambda)=\sum_i(c_i/\Lambda^{d_i-4})\,\mathcal{O}_i\), where each operator \(\mathcal{O}_i\) of mass dimension \(d_i>4\) is suppressed by powers of \(E/\Lambda\) and becomes negligible at low energies — this is the origin of decoupling. At each energy threshold where a heavy degree of freedom is integrated out, the Wilson coefficients \(c_i\) must satisfy matching conditions that relate the higher-resolution theory (above the threshold) to the lower-resolution one (below it): these matching conditions are the "comparison across refinements" of Section 1's definition of "uncuttable." No single effective Lagrangian at any finite cutoff \(\Lambda\) captures all physics; the full theory is the consistent tower of matched EFTs — the refinement limit — with the Wilson coefficients serving as the "extra data" required at each resolution level. The Standard Model itself is understood as an EFT valid below some new-physics threshold \(\Lambda_{\mathrm{NP}}\), with dimension-6 operators parametrizing deviations from unknown UV physics (the "Warsaw basis" of Grzadkowski et al., 2010), their coefficients probed by LHC measurements without knowing the UV completion.

`Remark 4.5 (Anomalies: topological constraints on the refinement limit).`
The counterterm towers, symmetry-preserving regulators, and RG flows of Sections 2–4 describe *how* to take the continuum limit. Anomalies constrain *which* continuum limits are consistent. The chiral anomaly (Adler, Bell, and Jackiw, 1969) is the prototype: a classically conserved axial current \(\partial_\mu j_5^\mu=0\) acquires a universal quantum correction \(\partial_\mu j_5^\mu=(e^2/16\pi^2)\,F_{\mu\nu}\tilde F^{\mu\nu}\) in the refinement limit. No single regularization scheme preserves all classical symmetries simultaneously: dimensional regularization breaks chiral symmetry (the \(\gamma_5\) problem in \(d\neq 4\)), Pauli–Villars breaks it through the regulator mass, and lattice regularization encounters the Nielsen–Ninomiya no-go theorem (1981) — a lattice fermion with exact chiral symmetry, locality, and the correct continuum limit is impossible. Yet the anomaly coefficient \(e^2/(16\pi^2)\) is universal: the same in every scheme, determined by topology (the index of the Dirac operator, Atiyah and Singer, 1968) rather than by any regularization detail. 't Hooft's anomaly matching condition (1980) elevates this to an RG constraint: anomaly coefficients computed from UV degrees of freedom must equal those computed from IR degrees of freedom, at every resolution scale. The physical consequence is that anomaly cancellation constrains which fermion content is consistent in a gauge theory — the Standard Model's particular arrangement of quarks and leptons (with their charges and color multiplicities) is precisely tuned to cancel all gauge anomalies, a topological consistency condition that no finite approximant determines but that the refinement limit must satisfy.

`Remark 4.6 (Conformal symmetry at RG fixed points: the self-determining refinement limit).`
At an RG fixed point \(\beta(g^*)=0\), the theory is scale-invariant — it looks identical at every resolution, the ultimate "refinement limit" of Remark 4.1. In \(d=2\), Zamolodchikov's \(c\)-theorem (1986) proves that a function \(c\) of the couplings decreases monotonically along RG flows and is stationary at fixed points, establishing both the irreversibility of coarse-graining (the "arrow of refinement") and, combined with unitarity, that scale invariance implies the larger conformal symmetry (the infinite-dimensional Virasoro algebra in \(d=2\); the finite-dimensional conformal group \(SO(d{+}1,1)\) in \(d \ge 3\)). In \(d=4\), Komargodski and Schwimmer (2011) proved the analogous \(a\)-theorem: the Euler anomaly coefficient \(a\) decreases monotonically between UV and IR fixed points. The irreversibility means that refinement loses degrees of freedom — there are fewer at long distances than at short — and the theory cannot flow back to its UV starting point. At the fixed point itself, the conformal bootstrap program uses only symmetry, unitarity, and crossing symmetry (no Lagrangian) to constrain the operator dimensions and structure constants, sometimes determining them to extraordinary precision (as in the 3D Ising universality class). This is the most extreme form of "the refinement limit determines itself": the self-consistency of the fixed point, rather than any specific microscopic Lagrangian, dictates the theory.

`Remark 4.7 (Asymptotic safety: non-perturbative refinement limits).`
Remarks 4.1–4.3 describe how the continuum limit is reached by tuning relevant couplings toward an RG fixed point — typically the Gaussian (free-field) fixed point, as in asymptotically free QCD. Weinberg (1979, General Relativity essay) proposed a more radical possibility: a theory may be UV-complete if its RG flow approaches a non-trivial (non-Gaussian) UV fixed point at which all essential couplings remain finite — "asymptotic safety." This applies even to theories that are perturbatively non-renormalizable: the mass dimension of Newton's constant is \(M^{2-d}\) in \(d\) dimensions (\(M^{-2}\) in \(d=4\)), making gravity power-counting non-renormalizable in the usual perturbative sense. Yet functional renormalization group (FRG) studies initiated by Reuter (1998) find numerical evidence for a non-Gaussian UV fixed point with a finite-dimensional critical surface — the number of relevant operators at the fixed point — meaning that only finitely many parameters (like a renormalizable theory) need to be measured to define the theory. The critical surface dimension plays the role of the "extra data" in Section 1's definition: it is the minimal specification needed for the refinement limit to converge to a unique theory. Whether this scenario is realized for gravity remains an open question (FRG evidence relies on truncations of the effective action), but the conceptual lesson is clear: the most ambitious form of "uncuttable" is a theory that IS its fixed point, defined not by any finite approximant or Lagrangian but by the self-consistency of the RG flow in the full, non-perturbative sense.

`Remark 4.8 (Dualities: multiple descriptions of the same "uncuttable" limit).`
Sections 2–3 show that different refinement prescriptions can produce different limits (Example 3.1, Remark 3.2). Dualities exhibit the complementary phenomenon: apparently different theories — different starting points for a "refinement" procedure — produce the same physics. The Kramers–Wannier duality (1941) maps the 2D Ising model at coupling \(K\) to coupling \(K^*\) satisfying \(\sinh(2K)\sinh(2K^*)=1\), exchanging the order variable (spin) and the disorder variable (domain wall). The self-dual point \(K_c=K_c^*\) coincides with the critical temperature — a fact special to the 2D Ising model, where the duality fixed point happens to be an RG fixed point of Remark 4.6. In string theory, T-duality maps a string compactified on a circle of radius \(R\) to one of radius \(\alpha'/R\), exchanging momentum modes \(n\) and winding modes \(w\); the self-dual radius \(R=\sqrt{\alpha'}\) (the string scale) acts as an effective minimum below which smaller circles become indistinguishable from larger ones. In gauge theory, the Montonen–Olive conjecture (1977) — extended to \(\mathcal{N}=4\) super-Yang–Mills, where it is conjectured to be an exact \(\mathrm{SL}(2,\mathbb{Z})\) symmetry acting on the complexified coupling \(\tau=\theta/(2\pi)+4\pi i/g^2\) — relates strong and weak coupling regimes, with strong evidence from BPS spectrum matching and topological field theory tests. These dualities illustrate that the "uncuttable" limit object (the physical theory) can be characterized from multiple, non-obviously equivalent starting points: different refinement paths converge to the same target, and identifying them as equivalent is itself part of the theory's definition.

`Remark 4.9 (Callan-Symanzik equation: refinement compatibility in differential form).`
Having surveyed the landscape from counterterms (Section 2) through Wilson's RG (Remark 4.1) to dualities (Remark 4.8), we note that the Callan-Symanzik equation (Callan, 1970; Symanzik, 1970) captures the differential essence of the entire discussion. The bare theory does not depend on the arbitrary renormalization scale \(\mu\); therefore, renormalized Green functions \(G^{(n)}\) satisfy a constraint: \([\mu\,\partial/\partial\mu + \beta(g)\,\partial/\partial g + n\gamma(g)]\,G^{(n)}=0\), where \(\beta(g)=\mu\,dg/d\mu\) is the beta function (the velocity field of the RG flow on coupling space) and \(\gamma(g)=(\mu/2)\,d(\ln Z)/d\mu\) is the anomalous dimension measuring the departure of the field scaling from its classical (engineering) dimension. The equation states: changing the "resolution" \(\mu\) must be compensated by flowing the coupling (\(\beta\)) and rescaling the field normalization (\(\gamma\)), so that the underlying physics is unchanged. This is the perturbative counterpart of Wilsonian refinement compatibility (Remark 4.1): \(\mu\) is a renormalization point rather than a physical resolution scale, but the structural message is the same — no single choice of \(\mu\) is the answer; the answer lives in the flow. The beta-function component appears already in the fermionic companion's Examples 5.1–5.2: the quadratic beta function \(\beta(g_R)\propto g_R^2\) of the 2D delta potential is the simplest realization of coupling flow under scale change. The anomalous dimension \(\gamma\) is the CS equation's additional ingredient, requiring nontrivial wavefunction renormalization; it gives power-law corrections to naive dimensional analysis — measurable, for instance, in the critical exponents of Remark 4.6.

`Remark 4.10 (Kadanoff block spins: the real-space prototype of refinement compatibility).`
The momentum-shell RG of Remark 4.1 has a real-space precursor: Kadanoff's block-spin transformation (1966). On a \(d\)-dimensional lattice with spacing \(a\), partition the sites into blocks of \(b^d\) spins and replace each block by a single effective spin (majority rule for discrete Ising spins, or a suitable coarse-graining for continuous fields). The blocked lattice has spacing \(ba\); rescale all lengths by \(1/b\) to restore the original spacing \(a\). The effective Hamiltonian at the blocked scale relates to the original by a renormalization transformation \(K'=R(K)\) in coupling space. Kadanoff's insight was the scaling hypothesis: self-similarity at the critical point implies that \(K_c\) is a fixed point of \(R\). Wilson (1971) transformed this into a complete framework by recognizing that \(R\) acts on the infinite-dimensional space of all possible Hamiltonians — not just on a few couplings — and that the classification of perturbations around a fixed point into relevant (growing under \(R\)), irrelevant (shrinking), and marginal (requiring higher-order analysis) operators determines which microscopic details survive in the continuum limit and which are washed out. Universality then follows: different microscopic Hamiltonians in the basin of attraction of the same fixed point share the same critical exponents, because these exponents are eigenvalues of the linearized RG transformation at the fixed point. This is refinement compatibility in reverse: instead of refining from coarse to fine (Section 2), one coarse-grains from fine to coarse and asks what is preserved — the answer is the fixed-point structure and the relevant-operator content, precisely the "extra data" that Section 1's definition of "uncuttable" requires.

`Remark 4.11 (Constructive QFT: do the refinement limits exist?).`
Remarks 4.1–4.10 describe the RG framework for taking continuum limits but do not address whether these limits rigorously exist as mathematical objects satisfying the Wightman axioms (or the equivalent Osterwalder–Schrader axioms for Euclidean theories). Constructive quantum field theory is the program to prove such existence. In d=2, Glimm and Jaffe (1968–1973) constructed the \(\varphi^4\) theory (more generally \(P(\varphi)_2\)) via Hamiltonian methods, with the cluster expansion of Glimm, Jaffe, and Spencer (1974) establishing the mass gap and infinite-volume limit; the Osterwalder–Schrader reconstruction theorem (1973, 1975) provided an alternative Euclidean route to the Wightman axioms (Glimm and Jaffe, 1987). In d=3, ultraviolet stability for \(\varphi^4_3\) was established by Feldman and Osterwalder (1976) and independently by Magnen and Sénéor (1976), with the full infinite-volume construction requiring further work through the late 1970s. The Gross–Neveu model in d=2 — the first rigorous construction of an asymptotically free field theory — was achieved by Gawędzki and Kupiainen (1985) and independently by Feldman, Magnen, Rivasseau, and Sénéor (1986). For these constructed theories, the perturbative series has been proved Borel summable to the exact answer, connecting the constructive results to the resummation methods of Remark 2.5. Two-dimensional Yang–Mills was solved exactly on the lattice (Migdal, 1975) and reformulated via topological methods (Witten, 1991), with rigorous continuum constructions by Driver (1989) and Sengupta (1997); however, this theory has no local propagating degrees of freedom and so provides a topological rather than dynamical test of the program. The central open problems are: (i) \(\varphi^4\) in d=4, where Aizenman (1981) and Fröhlich (1982) proved triviality in \(d\ge 5\) and established strong partial results in d=4 — lattice simulations and rigorous random-current bounds support the conclusion that only the free field survives the continuum limit, but a complete proof remains open; (ii) Yang–Mills in d=4 with mass gap, the Clay Millennium Prize problem of Remark 4.3, with Balaban's work (1982–1998) on ultraviolet stability of lattice gauge theories representing the closest rigorous approach. The structural lesson is that perturbative renormalizability (Remarks 4.1–4.2) guarantees a formal refinement limit order by order in the coupling, but the perturbative series is typically asymptotic (divergent); constructive QFT must establish existence by non-perturbative methods and then separately verify that the perturbative expansion is the asymptotic expansion of the constructed object — a qualitatively harder task than the convergence proofs of Section 2.

This note is therefore a small conceptual bridge: it isolates an early, analysis-level instance of the same meta-problem that reappears in quantization and in QFT.

# References

1. [FeynmanHibbs1965] Richard P. Feynman and Albert R. Hibbs, *Quantum Mechanics and Path Integrals*, McGraw-Hill, 1965. (Path integral as refinement limit of time-sliced amplitudes; foundational treatment.)
2. [Oksendal2003] Bernt Øksendal, *Stochastic Differential Equations: An Introduction with Applications*, 6th ed., Springer, 2003. ISBN `978-3-540-04758-2`. DOI `10.1007/978-3-642-14394-6`. (Standard textbook on Itô vs Stratonovich integrals and their relationship; used in uncuttable satellite for Remark 3.2.)
3. [Wilson1974] Kenneth G. Wilson, "Confinement of quarks," *Physical Review D* 10 (1974), 2445–2459. DOI `10.1103/PhysRevD.10.2445`. (Lattice gauge theory; used in Remark 4.3.)
4. [GrossWilczek1973] David J. Gross and Frank Wilczek, "Ultraviolet behavior of non-abelian gauge theories," *Physical Review Letters* 30 (1973), 1343–1346. DOI `10.1103/PhysRevLett.30.1343`. (Asymptotic freedom; used in Remark 4.3.)
5. [Politzer1973] H. David Politzer, "Reliable perturbative results for strong interactions?" *Physical Review Letters* 30 (1973), 1346–1349. DOI `10.1103/PhysRevLett.30.1346`. (Asymptotic freedom; used in Remark 4.3.)
6. [Weinberg1979] Steven Weinberg, "Phenomenological Lagrangians," *Physica A* 96 (1979), 327–340. DOI `10.1016/0378-4371(79)90223-1`. (Foundations of EFT; used in Remark 4.4.)
7. [Grzadkowski2010] B. Grzadkowski, M. Iskrzyński, M. Misiak, and J. Rosiek, "Dimension-six terms in the Standard Model Lagrangian," *JHEP* 2010:10 (2010), 085. arXiv:`1008.4884`. DOI `10.1007/JHEP10(2010)085`. (Warsaw basis for SM EFT dimension-6 operators; used in Remark 4.4.)
8. [Berry1991] M. V. Berry, "Asymptotics, superasymptotics, hyperasymptotics...," in *Asymptotics Beyond All Orders*, ed. H. Segur, S. Tanveer, H. Levine, Plenum, 1991, pp. 1–14. (Optimal truncation and superasymptotic accuracy; used in Remark 2.6.)
9. [tHooftVeltman1972] Gerard 't Hooft and Martinus J. G. Veltman, "Regularization and renormalization of gauge fields," *Nuclear Physics B* 44 (1972), 189–213. DOI `10.1016/0550-3213(72)90279-9`. (Dimensional regularization; used in Remark 4.2.)
10. [Reuter1998] Martin Reuter, "Nonperturbative evolution equation for quantum gravity," *Physical Review D* 57 (1998), 971–985. arXiv:`hep-th/9605030`. DOI `10.1103/PhysRevD.57.971`. (FRG evidence for asymptotic safety in gravity; used in Remark 4.7.)
11. [Kontsevich2003] Maxim Kontsevich, "Deformation quantization of Poisson manifolds," *Letters in Mathematical Physics* 66 (2003), 157–216. arXiv:`q-alg/9709040` (1997). DOI `10.1023/B:MATH.0000027508.00421.bf`. (Formality theorem; Fields Medal 1998; used in Remark 3.7.)
12. [AtiyahSinger1968] Michael F. Atiyah and Isadore M. Singer, "The index of elliptic operators: I," *Annals of Mathematics* 87 (1968), 484–530. DOI `10.2307/1970715`. (Index theorem; used in Remark 4.5.)
13. [Adler1969] Stephen L. Adler, "Axial-vector vertex in spinor electrodynamics," *Physical Review* 177 (1969), 2426–2438. DOI `10.1103/PhysRev.177.2426`. (Chiral anomaly; used in Remark 4.5.)
14. [BellJackiw1969] John S. Bell and Roman Jackiw, "A PCAC puzzle: \(\pi^0\to\gamma\gamma\) in the \(\sigma\)-model," *Il Nuovo Cimento A* 60 (1969), 47–61. DOI `10.1007/BF02823296`. (Chiral anomaly; used in Remark 4.5.)
15. [NielsenNinomiya1981] H. B. Nielsen and M. Ninomiya, "Absence of neutrinos on a lattice: I. Proof by homotopy theory," *Nuclear Physics B* 185 (1981), 20–40. DOI `10.1016/0550-3213(81)90361-8`. (No-go theorem for chiral lattice fermions; used in Remark 4.5.)
16. [tHooft1980] Gerard 't Hooft, "Naturalness, chiral symmetry, and spontaneous chiral symmetry breaking," in *Recent Developments in Gauge Theories*, ed. G. 't Hooft *et al.*, Plenum, 1980, pp. 135–157. ('t Hooft anomaly matching condition; used in Remark 4.5.)
17. [Zamolodchikov1986] Alexander B. Zamolodchikov, "'Irreversibility' of the flux of the renormalization group in a 2D field theory," *JETP Letters* 43 (1986), 730–732. (c-theorem; used in Remark 4.6.)
18. [KomargodskiSchwimmer2011] Zohar Komargodski and Adam Schwimmer, "On renormalization group flows in four dimensions," *JHEP* 2011:12 (2011), 099. arXiv:`1107.3987`. DOI `10.1007/JHEP12(2011)099`. (a-theorem proof; used in Remark 4.6.)
19. [Weinberg1979b] Steven Weinberg, "Ultraviolet divergences in quantum theories of gravitation," in *General Relativity: An Einstein Centenary Survey*, ed. S. W. Hawking and W. Israel, Cambridge University Press, 1979, pp. 790–831. (Asymptotic safety proposal; used in Remark 4.7.)
20. [RaySinger1971] D. B. Ray and I. M. Singer, "R-torsion and the Laplacian on Riemannian manifolds," *Advances in Mathematics* 7 (1971), 145–210. DOI `10.1016/0001-8708(71)90045-4`. (Spectral zeta function and functional determinant; used in Remark 2.7.)
21. [Hawking1977] S. W. Hawking, "Zeta function regularization of path integrals in curved spacetime," *Communications in Mathematical Physics* 55 (1977), 133–148. DOI `10.1007/BF01626516`. (Zeta regularization in quantum gravity context; used in Remark 2.7.)
22. [Ecalle1981] Jean Écalle, *Les fonctions résurgentes*, Volumes 1–3, Publications Mathématiques d'Orsay, 1981–1985. (Foundations of resurgence and alien calculus; used in Remark 2.8.)
23. [BenderWu1969] Carl M. Bender and Tai Tsun Wu, "Anharmonic oscillator," *Physical Review* 184 (1969), 1231–1260. DOI `10.1103/PhysRev.184.1231`. (Divergence of perturbation series; used in Remark 2.8.)
24. [BenderWu1973] Carl M. Bender and Tai Tsun Wu, "Anharmonic oscillator. II. A study of perturbation theory in large order," *Physical Review D* 7 (1973), 1620–1636. DOI `10.1103/PhysRevD.7.1620`. (Explicit large-order behavior; used in Remark 2.8.)
25. [KramersWannier1941] H. A. Kramers and G. H. Wannier, "Statistics of the two-dimensional ferromagnet. Part I," *Physical Review* 60 (1941), 252–262. DOI `10.1103/PhysRev.60.252`. (Kramers–Wannier duality; used in Remark 4.8.)
26. [MontonenOlive1977] C. Montonen and D. Olive, "Magnetic monopoles as gauge particles?" *Physics Letters B* 72 (1977), 117–120. DOI `10.1016/0370-2693(77)90076-4`. (Electric-magnetic duality conjecture; used in Remark 4.8.)
27. [Callan1970] Curtis G. Callan, Jr., "Broken scale invariance in scalar field theory," *Physical Review D* 2 (1970), 1541–1547. DOI `10.1103/PhysRevD.2.1541`. (Callan-Symanzik equation; used in Remark 4.9.)
28. [Symanzik1970] Kurt Symanzik, "Small distance behaviour in field theory and power counting," *Communications in Mathematical Physics* 18 (1970), 227–246. DOI `10.1007/BF01649434`. (Callan-Symanzik equation; used in Remark 4.9.)
29. [Kadanoff1966] Leo P. Kadanoff, "Scaling laws for Ising models near \(T_c\)," *Physics (Long Island City, N.Y.)* 2 (1966), 263–272. DOI `10.1103/PhysicsPhysiqueFizika.2.263`. (Block-spin scaling hypothesis; used in Remark 4.10.)
30. [Wilson1971] Kenneth G. Wilson, "Renormalization group and critical phenomena. I. Renormalization group and the Kadanoff scaling picture," *Physical Review B* 4 (1971), 3174–3183. DOI `10.1103/PhysRevB.4.3174`. (Quantitative RG framework, fixed-point classification; used in Remark 4.10.)
31. [vonNeumann1931] John von Neumann, "Die Eindeutigkeit der Schrödingerschen Operatoren," *Mathematische Annalen* 104 (1931), 570–578. DOI `10.1007/BF01457956`. (Uniqueness of CCR representations; used in Remark 3.8.)
32. [Haag1955] Rudolf Haag, "On quantum field theories," *Matematisk-fysiske Meddelelser* 29, no. 12 (1955), 1–37. (Inequivalence of free and interacting representations; used in Remark 3.8.)
33. [GlimmJaffe1987] James Glimm and Arthur Jaffe, *Quantum Physics: A Functional Integral Point of View*, 2nd ed., Springer, 1987. ISBN `978-0-387-96476-8`. (Canonical reference for constructive QFT; used in Remark 4.11.)
34. [Aizenman1981] Michael Aizenman, "Proof of the triviality of \(\varphi^4_d\) field theory and some mean-field features of Ising models for \(d>4\)," *Physical Review Letters* 47 (1981), 1–4. DOI `10.1103/PhysRevLett.47.1`. (Triviality of \(\varphi^4\) in \(d\ge 5\); used in Remark 4.11.)
35. [Frohlich1982] Jürg Fröhlich, "On the triviality of \(\lambda\varphi^4_d\) theories and the approach to the critical point in \(d\ge 4\) dimensions," *Nuclear Physics B* 200 (1982), 281–296. DOI `10.1016/0550-3213(82)90088-8`. (Companion triviality result; used in Remark 4.11.)
36. [Dedekind1872] Richard Dedekind, *Stetigkeit und irrationale Zahlen* (Continuity and Irrational Numbers), 1872. English translation in *Essays on the Theory of Numbers*, Dover, 1963. (Order-theoretic construction of the reals via cuts; used in Remark 1.3.)
37. [Kac1949] Mark Kac, "On distributions of certain Wiener functionals," *Transactions of the American Mathematical Society* 65 (1949), 1–13. DOI `10.2307/1990512`. (Feynman–Kac formula; used in Remark 3.9.)
38. [Simon1979] Barry Simon, *Functional Integration and Quantum Physics*, Academic Press, 1979. ISBN `978-0-12-644250-0`. (Standard reference for the Feynman–Kac formula and Euclidean path integrals; used in Remark 3.9.)
