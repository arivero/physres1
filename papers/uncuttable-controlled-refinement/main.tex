% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={``Uncuttable'' as Controlled Refinement},
  pdfauthor={Alejandro Rivero},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{``Uncuttable'' as Controlled Refinement}
\author{Alejandro Rivero}
\date{2026}

\begin{document}
\maketitle
\begin{abstract}
This note fixes a meaning of ``uncuttable'' aligned with the
refinement-compatibility thesis of the cornerstone manuscript. Here
\textbf{uncuttable} does not mean ``indivisible.'' It means: the
quantity of interest is not determined by any \emph{finite} dissection
alone; it is a \textbf{limit object} whose definition requires a
refinement rule and a comparison structure across refinements.

The point is structural and mathematical: once a theory is built from
composable local pieces, the continuum theory is the stable target of a
refinement limit, and extra control data may be required for that limit
to exist or be unique.
\end{abstract}

\hypertarget{definition}{%
\section{1. Definition}\label{definition}}

Call a quantity \(Q\) \textbf{uncuttable} (in this note's sense) if: 1.
there exists a family of finite approximants \(Q_N\) produced by a
finite dissection/refinement scheme of depth \(N\), but 2. the value of
interest is not any finite \(Q_N\); it is a controlled limit
\(Q=\lim_{N\to\infty}Q_N\), and 3. specifying the \emph{rule of
refinement} and the \emph{comparison across refinements} is part of the
definition of \(Q\).

The historical resonance is deliberate. The Greek
\(\acute\alpha\tau o\mu o\varsigma\) (``a-tomos,'' uncuttable) was
coined by Leucippus and Democritus to denote indivisible substance ---
matter that cannot be divided further. The shift proposed here is from
ontology to procedure: what is ``uncuttable'' is not a smallest piece of
stuff, but a limit object that no single finite dissection captures. The
indivisibility is not in the substance but in the definition: you cannot
``cut'' the limit into finitely many pieces and recover it without
specifying how the pieces are to be reassembled and refined.

This is the ordinary situation in analysis: finite partitions
approximate, but the object is defined by a limiting procedure together
with hypotheses that ensure convergence/uniqueness.

\hypertarget{toy-model-an-integral-is-already-a-refinement-limit}{%
\section{2. Toy model: an integral is already a refinement
limit}\label{toy-model-an-integral-is-already-a-refinement-limit}}

Let \(f:[a,b]\to\mathbb R\). A prototypical refinement family is a
partition \(P_N=\{a=t_0<\cdots<t_N=b\}\) with mesh
\(\|P_N\|:=\max_k(t_{k+1}-t_k)\to 0\). Define the Riemann-sum
approximants \[
Q_N:=\sum_{k=0}^{N-1} f(\xi_k)\,(t_{k+1}-t_k),
\qquad \xi_k\in[t_k,t_{k+1}].
\] In good cases, \(Q_N\to \int_a^b f(t)\,dt\) as \(\|P_N\|\to 0\), and
the limit is independent of the tags \(\xi_k\). But this is not a
tautology: the limit can fail to exist or can depend on the refinement
rule unless hypotheses are stated.

A concrete failure case appears already in calculus. Consider the
difference quotient \(f(x+\varepsilon)/\varepsilon\): for each finite
\(\varepsilon\), the quantity diverges as \(\varepsilon\to 0\). The
``refinement limit'' exists only after subtracting a counterterm
\(f(x)/\varepsilon\), yielding the derivative \(f'(x)\). The subtraction
is part of the definition of the limit --- without it, the refinement
procedure does not converge. This is the simplest model of
renormalization: a ``cut'' at finite \(\varepsilon\) is not the answer;
the answer requires a controlled \(\varepsilon\to 0\) limit with
explicit subtraction rules.

In the present program, this is the basic moral: finite cuts
approximate, but the value is defined by \textbf{controlled refinement}.

\hypertarget{worked-example-the-derivative-as-a-renormalized-refinement-limit}{%
\subsection{2.5 Worked example: the derivative as a renormalized
refinement
limit}\label{worked-example-the-derivative-as-a-renormalized-refinement-limit}}

The paragraph above describes the difference-quotient subtraction
informally. We now make it explicit as a minimal model of
renormalization.

\texttt{Example\ 2.1\ (Derivative\ as\ counterterm-subtracted\ limit).}
Let \(f:\mathbb{R}\to\mathbb{R}\) be differentiable at \(x\). The naive
``refinement'' approximant at scale \(\varepsilon\) is \[
R(\varepsilon):=\frac{f(x+\varepsilon)}{\varepsilon}.
\] This diverges as \(\varepsilon\to 0\) whenever \(f(x)\neq 0\). To
extract a finite limit, subtract a counterterm: \[
R_{\mathrm{ren}}(\varepsilon):=\frac{f(x+\varepsilon)-f(x)}{\varepsilon}.
\] Now \(\lim_{\varepsilon\to 0}R_{\mathrm{ren}}(\varepsilon)=f'(x)\),
which is finite and independent of the ``regulator'' \(\varepsilon\).
The structure is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Regulated quantity}: \(R(\varepsilon)\), finite for each
  \(\varepsilon>0\) but divergent as \(\varepsilon\to 0\).
\item
  \textbf{Counterterm}: \(f(x)/\varepsilon\), which absorbs the
  divergence.
\item
  \textbf{Renormalized observable}: \(f'(x)\), the
  \(\varepsilon\)-independent limit.
\end{enumerate}

This is formally identical to the pattern in perturbative
renormalization: a bare quantity diverges as the regulator is removed,
but a systematic subtraction yields a finite, physically meaningful
result. The subtraction rule is not ad hoc; it is forced by the
requirement that the result depend smoothly on the data and be
independent of the regulator.

\texttt{Remark\ 2.2\ (Higher-order\ counterterms).} If \(f\) is \(C^n\),
the Taylor expansion \[
f(x+\varepsilon)=f(x)+f'(x)\varepsilon+\tfrac{1}{2}f''(x)\varepsilon^2+\cdots+\tfrac{1}{n!}f^{(n)}(x)\varepsilon^n+o(\varepsilon^n)
\] provides a systematic tower of subtractions, one per order. Removing
terms through order \(k-1\) and dividing by \(\varepsilon^k\) gives the
\(k\)-th Taylor coefficient \(f^{(k)}(x)/k!\) plus vanishing corrections
--- or equivalently, dividing by \(\varepsilon^k/k!\) recovers the
derivative \(f^{(k)}(x)\) itself. Each order is a
``counterterm-subtracted refinement limit'' --- the analog of
loop-by-loop renormalization in QFT.

\texttt{Remark\ 2.3\ (Euler–Maclaurin:\ correction\ tower\ for\ the\ Riemann\ sum).}
The same pattern applies to the integral approximation itself. The
Euler--Maclaurin formula expresses the error of the left-point Riemann
sum as a systematic expansion in the mesh size \(h=(b-a)/N\): \[
\sum_{k=0}^{N-1}f(a+kh)\,h = \int_a^b f(t)\,dt + \sum_{j=1}^{p}\frac{B_j}{j!}\,h^j\,\bigl[f^{(j-1)}(b)-f^{(j-1)}(a)\bigr] + O(h^{p+1}),
\] where \(B_j\) are Bernoulli numbers
(\(B_1=-\tfrac12,\;B_2=\tfrac16,\;B_3=0,\ldots\)). Each correction
involves a power of the mesh (the ``regulator''), multiplied by endpoint
derivatives (boundary data). The leading correction \(j=1\) converts the
left-point sum into the trapezoidal rule; the \(j=2\) term adds an
endpoint-derivative correction that raises the order to \(O(h^4)\). This
is the interval counterpart of Remark 2.2's single-point tower: there
the subtractions were local (Taylor coefficients at \(x\)); here they
are boundary-localized (endpoint derivatives of \(f\)).

\texttt{Remark\ 2.4\ (Richardson\ extrapolation:\ subtracting\ leading\ errors\ across\ refinement\ levels).}
Given an approximation family \(Q(h)=Q+c_1 h^p+O(h^{p+1})\), the
combination \(R(h)=(2^p\,Q(h/2)-Q(h))/(2^p-1)\) eliminates the leading
error term without knowledge of \(c_1\) --- only the exponent \(p\) is
needed. Iterating at mesh sizes \(h, h/2, h/4,\ldots\) produces a
triangular tableau (Romberg) that peels off one order of correction per
column, mirroring the counterterm tower of Remark 2.2. Applied to the
trapezoidal rule (whose even-power error expansion follows from Remark
2.3 with \(B_{2k+1}=0\) for \(k\ge1\)), the first Richardson step
(\(p=2\)) yields Simpson's rule; the second (\(p=4\)) yields Boole's
rule --- each time eliminating the next Euler--Maclaurin correction
without computing endpoint derivatives. The structural parallel to
minimal subtraction in dimensional regularization is precise: the
exponent \(p\) plays the role of the pole order, and the subtraction
procedure is universal --- independent of the specific integrand.

\texttt{Remark\ 2.5\ (Padé\ approximants\ and\ Borel\ resummation:\ controlled\ extrapolation\ beyond\ convergence).}
The polynomial counterterm towers of Remarks 2.2--2.4 fail when the
series itself diverges. Padé approximants --- rational functions
\([L/M]=P_L(x)/Q_M(x)\) whose Taylor expansion matches a given series
through order \(x^{L+M}\) --- extend the strategy to meromorphic
functions: the Montessus de Ballore theorem guarantees convergence
outside the Taylor disk when singularities are isolated poles. For
asymptotic (divergent) series with factorial coefficient growth, the
Borel transform \(B(t)=\sum (c_n/n!)\,t^n\) has finite radius of
convergence, and the Borel--Padé method --- Padé-approximating \(B(t)\)
and Laplace-transforming back --- extracts a finite, well-defined
function from a formally divergent expansion. This is the ``uncuttable''
pattern at the level of series: the naive sum has no limit, but a
systematic resummation prescription (transform, rational approximation,
inverse transform) is part of the definition of the intended quantity.

\texttt{Remark\ 2.6\ (Optimal\ truncation:\ the\ exponential\ accuracy\ barrier).}
For an asymptotic (divergent) series whose coefficients grow as
\(|a_n|\sim C\,n!/\rho^n\), the \(n\)-th term is minimized near
\(n\approx\rho/x\). Truncating at this optimal index --- the smallest
term --- yields an approximation with exponentially small error
\(\sim e^{-\rho/x}\), far better than any fixed algebraic order. This
``superasymptotic'' accuracy (Berry, 1991) is the best any finite
truncation can achieve; the exponentially small remainder is precisely
the non-perturbative content (instantons, tunneling amplitudes) that the
Borel resummation of Remark 2.5 recovers. Optimal truncation thus marks
the boundary between what finite ``cuts'' can reach and what requires
the full resummation machinery: the ultimate illustration that the
intended quantity is not captured by any single approximant but lives in
the controlled limit.

\hypertarget{dynamics-action-stationarity-and-the-need-for-control-data}{%
\section{3. Dynamics: action, stationarity, and the need for control
data}\label{dynamics-action-stationarity-and-the-need-for-control-data}}

The cornerstone manuscript uses the same template in mechanics. Given a
partition of time, the discrete action \[
S_N[q]=\sum_k \mathcal L\!\left(q_k,\frac{q_{k+1}-q_k}{\Delta t_k},t_k\right)\Delta t_k
\] is a finite refinement approximant. The continuum action
\(S[q]=\int \mathcal L\,dt\) is a refinement limit.

Two ``uncuttable'' features appear immediately when one pushes beyond
smooth classical paths: 1. \textbf{Singular probes and corners:}
stationarity must be interpreted in weak/distributional form;
point-supported variations require mollification. 2.
\textbf{Non-uniqueness of refinement schemes:} different discretization
conventions (even if classically equivalent) can produce distinct
refined objects unless an equivalence or control map is specified.

These are exactly the obstructions discussed in the cornerstone
manuscript: the point is not indivisible atoms, but limit control.

In the quantum setting, the ``uncuttable'' character becomes sharper.
The path-integral amplitude \[
K(q_f,t_f;q_i,t_i)=\int \prod_{k=1}^{N-1}dq_k\;\prod_{k=0}^{N-1}K_\Delta(q_{k+1},q_k;t_k)
\] is a product of short-time kernels composed over a time partition of
depth \(N\). No finite \(N\) gives the exact propagator; the propagator
is the \(N\to\infty\) refinement limit. Crucially, the control parameter
\(\hbar\) enters the short-time kernels as \(\exp(iS_\Delta/\hbar)\),
and different discretization conventions (left-point, midpoint,
symmetric) can produce distinct \(O(\hbar)\) corrections even though
they share the same classical \(\hbar\to 0\) limit
{[}FeynmanHibbs1965{]}. Thus the quantum amplitude is doubly
``uncuttable'': it requires both a refinement rule (time-slicing
prescription) and a comparison/equivalence structure (ordering
convention or half-density normalization) before the limit is
well-defined.

\texttt{Example\ 3.1\ (Non-uniqueness\ of\ refinement:\ \textbackslash{}(\textbackslash{}alpha\textbackslash{})-ordering).}
For the classical Hamiltonian \(H(q,p)=qp\), different time-slicing
prescriptions --- evaluating position at
\(q_\alpha=(1-\alpha)q_k+\alpha q_{k+1}\) in each slice --- produce
different quantum operators: \[
\hat H_\alpha=\alpha\hat q\hat p + (1-\alpha)\hat p\hat q = \hat p\hat q + \alpha\,i\hbar.
\] At \(\alpha=0\) (prepoint): \(\hat p\hat q\). At \(\alpha=1/2\)
(midpoint/Weyl): \(\tfrac12(\hat q\hat p+\hat p\hat q)\). At
\(\alpha=1\) (postpoint): \(\hat q\hat p\). All three share the
classical limit \(H=qp\) as \(\hbar\to0\), but they are distinct quantum
objects. The ``uncuttable'' message: the continuum quantum Hamiltonian
is not determined by any single finite time-slicing; it requires
specifying the refinement convention \(\alpha\) as part of the
definition.

\texttt{Remark\ 3.2\ (Stochastic\ counterpart:\ Itô\ versus\ Stratonovich).}
The same refinement non-uniqueness appears in stochastic calculus. For a
Wiener process \(W_t\), the stochastic integral \(\int f(W_t)\,dW_t\)
can be defined using left-point (Itô) or midpoint (Stratonovich)
evaluation in the Riemann sums: \[
\text{Itô:}\;\sum_k f(W_{t_k})\,\Delta W_k,\qquad
\text{Stratonovich:}\;\sum_k f\!\left(\tfrac{W_{t_k}+W_{t_{k+1}}}{2}\right)\Delta W_k.
\] Both converge as mesh \(\to 0\) (for \(f\in C^2\)), but to different
limits related by
\(\int f(W)\circ dW = \int f(W)\,dW + \tfrac12\int f'(W)\,dt\)
{[}Oksendal2003{]}. The correction \(\tfrac12 f'\,dt\) arises because
Brownian paths have non-zero quadratic variation
(\(\sum(\Delta W_k)^2\to T\neq 0\)); for paths of bounded variation, the
quadratic variation vanishes and all evaluation-point prescriptions
agree. This parallels Example 3.1: the ordering correction
\(\alpha\,i\hbar\) vanishes when \(\hbar\to 0\) (smooth classical
paths), but is unavoidable at finite \(\hbar\). Both cases instantiate
the ``uncuttable'' pattern: when paths are rough enough, the refinement
prescription becomes part of the definition.

\texttt{Remark\ 3.3\ (Trotter\ product\ formula\ as\ a\ refinement\ theorem).}
The mathematical backbone of the path-integral refinement limit is the
Trotter product formula: for self-adjoint operators \(A\) and \(B\) with
\(A+B\) essentially self-adjoint on a common dense domain, \[
e^{t(A+B)}=\lim_{N\to\infty}\left(e^{tA/N}\,e^{tB/N}\right)^N.
\] In the path-integral context, \(A=-i\hat T/\hbar\) (kinetic) and
\(B=-i\hat V/\hbar\) (potential), so each factor is a free propagation
or a potential phase-kick at one time slice. The formula states that the
exact propagator is a refinement limit of \(N\)-fold compositions ---
and that this limit converges, with total error \(O(1/N)\). The
symmetric (Suzuki--Trotter) splitting \(e^{tA/(2N)}e^{tB/N}e^{tA/(2N)}\)
reduces the total error to \(O(1/N^2)\) by canceling the leading
Baker--Campbell--Hausdorff commutator \([A,B]\) contribution at each
step. This is the ``uncuttable'' pattern in operator-algebraic form: no
finite product equals \(e^{t(A+B)}\), and the rate of convergence
depends on controlling the non-commutativity of the pieces
{[}FeynmanHibbs1965{]}.

\texttt{Remark\ 3.4\ (Symplectic\ integrators:\ structural\ compatibility\ at\ finite\ resolution).}
The symmetric Trotter splitting is the operator form of the
Störmer--Verlet (leapfrog) integrator. The discrete map is
\emph{exactly} symplectic at every finite \(N\): it preserves the
Poisson brackets (or, quantum-mechanically, unitarity) not just
asymptotically but at each approximation level. Backward error analysis
shows that the numerical flow is the exact flow of a nearby ``shadow
Hamiltonian'' \(\tilde H = H + O(\Delta t^2)\), which is conserved
exactly along the discrete orbit; the original \(H\) oscillates with
amplitude \(O(\Delta t^2)\) without secular drift. Higher-order
symplectic splittings (Yoshida, Forest--Ruth) systematically cancel
further Baker--Campbell--Hausdorff commutators, paralleling the
counterterm tower of Remark 2.2. This is refinement compatibility made
precise: the approximants at every level carry a structural invariant
(symplecticity) that the continuum limit inherits.

\texttt{Remark\ 3.5\ (Adiabatic\ limit:\ Berry\ phase\ as\ geometric\ refinement\ correction).}
The adiabatic theorem is another instance of the same refinement
pattern. A slowly varying Hamiltonian \(H(t/T)\) with \(T\to\infty\) has
instantaneous eigenstates as its finite approximants; the exact
evolution is the refinement limit, controlled by the gap condition
(energy gaps bounded away from zero). Diabatic transitions are
suppressed as \(T\to\infty\), but a geometric correction survives
exactly: the Berry phase
\(\gamma_n = i\oint\langle n|\nabla_R n\rangle\cdot dR\), which is the
holonomy of a U(1) connection on the eigenstate bundle over parameter
space. This is the ``extra structure'' of Section 4 in geometric dress
--- a connection datum that no local approximant captures, yet persists
through the refinement limit and can be topologically quantized.

\texttt{Remark\ 3.6\ (WKB\ approximation:\ \textbackslash{}(\textbackslash{}hbar\textbackslash{})-refinement\ with\ connection-formula\ control\ data).}
The WKB ansatz \(\psi(x)=A(x)\exp(iS(x)/\hbar)\) generates a formal
expansion of \(S\) in powers of \(\hbar\), with the leading term
satisfying the Hamilton--Jacobi equation (classical mechanics) and each
subsequent order adding a quantum correction --- a refinement family
indexed by truncation depth. At classical turning points, the WKB
amplitude \(A\propto 1/\sqrt{p(x)}\) diverges: the local approximation
breaks down, and a connection formula (obtained by matching to an Airy
solution) is needed to relate oscillatory and evanescent regions. This
connection formula is the ``control data'' for the global WKB solution,
and the requirement that the wavefunction be single-valued around a
closed orbit produces the Bohr--Sommerfeld quantization condition
\(\oint p\,dq=(n+\tfrac12\mu)\,\hbar\), where \(\mu\) is the Maslov
index counting turning points --- a topological invariant that, like the
Berry phase in Remark 3.5, no local WKB expansion can determine.

\hypertarget{outlook-refinement-compatibility-as-the-extra-structure}{%
\section{4. Outlook: refinement compatibility as ``the extra
structure''}\label{outlook-refinement-compatibility-as-the-extra-structure}}

In the companion papers, the ``extra structure'' used to control
refinement limits is made explicit: - half-densities make kernel
composition coordinate-free without hidden measure choices, - control
maps \(\tau\) encode how parameters must flow under refinement to
maintain stability, - renormalization is the compatibility rule when
naive refinement limits diverge.

\texttt{Remark\ 4.1\ (Wilson\textquotesingle{}s\ renormalization\ group:\ the\ definitive\ refinement\ framework).}
The counterterm towers of Section 2 and the refinement-convention
dependence of Section 3 find their definitive home in Wilson's
renormalization group (1971, 1974). Wilson's blocking transformation ---
integrating out short-distance degrees of freedom, rescaling, and
absorbing the result into effective couplings --- is literally a
refinement map on the space of theories. Relevant operators (those that
grow under the RG flow) are the counterterms that must be tuned to reach
a well-defined continuum limit; irrelevant operators shrink and their
effects vanish --- this is the origin of universality, the statement
that the continuum limit depends only on a few relevant couplings, not
on microscopic lattice details. The continuum QFT is not ``what you get
when the lattice spacing goes to zero'' but the theory defined by the
vicinity of an RG fixed point, the self-consistent target toward which
all sufficiently fine approximants flow. This is the ``uncuttable''
pattern at full strength: no finite lattice is the theory, but the
theory is the stable limit of the refinement flow, defined by the
fixed-point structure and the relevant couplings that parameterize
departures from it.

\texttt{Remark\ 4.2\ (Dimensional\ regularization:\ refinement-scheme\ independence\ made\ explicit).}
The counterterm towers of Section 2 use a momentum cutoff \(\Lambda\) as
the finite-resolution regulator. Dimensional regularization ('t Hooft
and Veltman, 1972) provides a structurally different scheme: loop
integrals are evaluated in \(d=4-\varepsilon\) dimensions, and UV
divergences appear as poles in \(1/\varepsilon\) rather than as
\(\Lambda\to\infty\) power laws. The minimal subtraction prescription
(MS-bar) --- subtracting the \(1/\varepsilon\) pole together with
universal constants \(\ln(4\pi)-\gamma_E\) --- is the counterterm
convention, analogous to the evaluation-point convention \(\alpha\) of
Example 3.1. At \(L\) loops, poles up to \(1/\varepsilon^L\) arise from
nested subdivergences, and Zimmermann's forest formula (the BPHZ
recursive subtraction scheme) ensures that order-by-order pole
subtraction leaves a finite result --- the dim-reg version of the
counterterm tower in Remark 2.2. Crucially, cutoff and dim-reg
regularization produce the same physical S-matrix elements: the
renormalized observables are independent of the specific regulator, just
as the integral in Section 2 is independent of the partition rule.
Dimensional regularization additionally preserves gauge invariance
(Ward-Takahashi identities) and Lorentz covariance, structural
invariants that a naive momentum cutoff would break --- paralleling the
symplectic integrators of Remark 3.4, which preserve Hamiltonian
structure that generic discretizations destroy.

\texttt{Remark\ 4.3\ (Lattice\ gauge\ theory:\ the\ definitive\ finite-resolution\ approximant).}
Wilson's lattice gauge theory (1974) provides the most concrete physical
realization of the ``uncuttable'' pattern. Spacetime is discretized on a
hypercubic lattice with spacing \(a\); the gauge field lives on links as
group elements \(U_\mu(x)\in SU(N)\), and the action is built from the
smallest closed loops (plaquettes):
\(S_W=\beta\sum_P\mathrm{Re}\,\mathrm{Tr}(1-U_P)\) with
\(\beta=2N/g^2\). Crucially, this formulation maintains exact gauge
invariance at every lattice spacing --- the action is a sum of traces of
closed loops, which are invariant under local gauge transformations ---
paralleling the symplectic integrators of Remark 3.4, which preserve
Hamiltonian structure at every finite time step. The continuum limit
\(a\to 0\) requires tuning the bare coupling \(g(a)\) along the critical
surface \(\beta\to\infty\); for non-abelian theories with
\(\beta_0=(11N-2N_f)/3>0\), asymptotic freedom (Gross--Wilczek and
Politzer, 1973) guarantees that \(g(a)\to 0\) as \(a\to 0\), making this
tuning controllable. No finite lattice is the theory: the continuum QCD
is the refinement limit, and proving that confinement (the area law for
large Wilson loops) and a mass gap survive this limit is the content of
the Clay Millennium Prize problem for Yang--Mills theory.

\texttt{Remark\ 4.4\ (Effective\ field\ theory:\ the\ systematic\ refinement\ hierarchy).}
The counterterm towers, refinement conventions, and RG flows of Sections
2--4 are unified in the effective field theory (EFT) framework
(Weinberg, 1979). EFT organizes physical predictions as a tower of
Lagrangians
\(\mathcal{L}_{\mathrm{EFT}}(\Lambda)=\sum_i(c_i/\Lambda^{d_i-4})\,\mathcal{O}_i\),
where each operator \(\mathcal{O}_i\) of mass dimension \(d_i>4\) is
suppressed by powers of \(E/\Lambda\) and becomes negligible at low
energies --- this is the origin of decoupling. At each energy threshold
where a heavy degree of freedom is integrated out, the Wilson
coefficients \(c_i\) must satisfy matching conditions that relate the
higher-resolution theory (above the threshold) to the lower-resolution
one (below it): these matching conditions are the ``comparison across
refinements'' of Section 1's definition of ``uncuttable.'' No single
effective Lagrangian at any finite cutoff \(\Lambda\) captures all
physics; the full theory is the consistent tower of matched EFTs --- the
refinement limit --- with the Wilson coefficients serving as the ``extra
data'' required at each resolution level. The Standard Model itself is
understood as an EFT valid below some new-physics threshold
\(\Lambda_{\mathrm{NP}}\), with dimension-6 operators parametrizing
deviations from unknown UV physics (the ``Warsaw basis'' of Grzadkowski
et al., 2010), their coefficients probed by LHC measurements without
knowing the UV completion.

This note is therefore a small conceptual bridge: it isolates an early,
analysis-level instance of the same meta-problem that reappears in
quantization and in QFT.

\hypertarget{references}{%
\section{References}\label{references}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  {[}FeynmanHibbs1965{]} Richard P. Feynman and Albert R. Hibbs,
  \emph{Quantum Mechanics and Path Integrals}, McGraw-Hill, 1965. (Path
  integral as refinement limit of time-sliced amplitudes; foundational
  treatment.)
\item
  {[}Oksendal2003{]} Bernt Øksendal, \emph{Stochastic Differential
  Equations: An Introduction with Applications}, 6th ed., Springer,
  2003. ISBN \texttt{978-3-540-04758-2}. DOI
  \texttt{10.1007/978-3-642-14394-6}. (Standard textbook on Itô vs
  Stratonovich integrals and their relationship; used in uncuttable
  satellite for Remark 3.2.)
\end{enumerate}

\end{document}
